{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd3b860-c21f-4d00-a942-e904403d59fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 13:07:58,210 - INFO - Using device: cpu\n",
      "2024-12-14 13:07:58,647 - INFO - Starting training...\n",
      "2024-12-14 13:09:12,084 - INFO - Epoch [1/5], Batch [100], Loss: 2.1142\n",
      "2024-12-14 13:10:15,437 - INFO - Epoch [1/5], Batch [200], Loss: 1.8624\n",
      "2024-12-14 13:11:20,916 - INFO - Epoch [1/5], Batch [300], Loss: 1.7984\n",
      "2024-12-14 13:12:22,599 - INFO - Epoch [1/5], Batch [400], Loss: 1.7352\n",
      "2024-12-14 13:13:24,197 - INFO - Epoch [1/5], Batch [500], Loss: 1.7251\n",
      "2024-12-14 13:14:31,789 - INFO - Epoch [1/5], Batch [600], Loss: 1.6517\n",
      "2024-12-14 13:15:26,400 - INFO - Epoch [1/5], Batch [700], Loss: 1.5811\n",
      "2024-12-14 13:16:21,842 - INFO - Epoch [1/5], Batch [800], Loss: 1.6062\n",
      "2024-12-14 13:17:18,447 - INFO - Epoch [1/5], Batch [900], Loss: 1.5384\n",
      "2024-12-14 13:18:26,230 - INFO - Epoch [1/5], Batch [1000], Loss: 1.5518\n",
      "2024-12-14 13:19:21,549 - INFO - Epoch [1/5], Batch [1100], Loss: 1.5201\n",
      "2024-12-14 13:20:14,685 - INFO - Epoch [1/5], Batch [1200], Loss: 1.4574\n",
      "2024-12-14 13:21:10,157 - INFO - Epoch [1/5], Batch [1300], Loss: 1.4070\n",
      "2024-12-14 13:22:18,680 - INFO - Epoch [1/5], Batch [1400], Loss: 1.4335\n",
      "2024-12-14 13:23:27,873 - INFO - Epoch [1/5], Batch [1500], Loss: 1.4210\n",
      "2024-12-14 13:24:12,615 - INFO - Evaluating after epoch 1...\n",
      "2024-12-14 13:24:46,189 - INFO - Test Accuracy: 52.17%\n",
      "2024-12-14 13:25:52,748 - INFO - Epoch [2/5], Batch [100], Loss: 1.5473\n",
      "2024-12-14 13:27:02,610 - INFO - Epoch [2/5], Batch [200], Loss: 1.4383\n",
      "2024-12-14 13:28:07,524 - INFO - Epoch [2/5], Batch [300], Loss: 1.3769\n",
      "2024-12-14 13:29:13,335 - INFO - Epoch [2/5], Batch [400], Loss: 1.3617\n",
      "2024-12-14 13:30:18,233 - INFO - Epoch [2/5], Batch [500], Loss: 1.3515\n",
      "2024-12-14 13:31:30,161 - INFO - Epoch [2/5], Batch [600], Loss: 1.3684\n",
      "2024-12-14 13:32:37,584 - INFO - Epoch [2/5], Batch [700], Loss: 1.3618\n",
      "2024-12-14 13:33:43,119 - INFO - Epoch [2/5], Batch [800], Loss: 1.3193\n",
      "2024-12-14 13:34:36,015 - INFO - Epoch [2/5], Batch [900], Loss: 1.3339\n",
      "2024-12-14 13:35:33,351 - INFO - Epoch [2/5], Batch [1000], Loss: 1.2645\n",
      "2024-12-14 13:36:39,084 - INFO - Epoch [2/5], Batch [1100], Loss: 1.3194\n",
      "2024-12-14 13:37:44,834 - INFO - Epoch [2/5], Batch [1200], Loss: 1.2650\n",
      "2024-12-14 13:38:58,052 - INFO - Epoch [2/5], Batch [1300], Loss: 1.2751\n",
      "2024-12-14 13:39:51,784 - INFO - Epoch [2/5], Batch [1400], Loss: 1.2222\n",
      "2024-12-14 13:40:42,707 - INFO - Epoch [2/5], Batch [1500], Loss: 1.2622\n",
      "2024-12-14 13:41:23,749 - INFO - Evaluating after epoch 2...\n",
      "2024-12-14 13:41:55,040 - INFO - Test Accuracy: 57.06%\n",
      "2024-12-14 13:43:10,936 - INFO - Epoch [3/5], Batch [100], Loss: 1.2052\n",
      "2024-12-14 13:44:12,724 - INFO - Epoch [3/5], Batch [200], Loss: 1.2341\n",
      "2024-12-14 13:45:15,869 - INFO - Epoch [3/5], Batch [300], Loss: 1.1738\n",
      "2024-12-14 13:46:25,131 - INFO - Epoch [3/5], Batch [400], Loss: 1.1585\n",
      "2024-12-14 13:47:33,683 - INFO - Epoch [3/5], Batch [500], Loss: 1.1845\n",
      "2024-12-14 13:48:28,208 - INFO - Epoch [3/5], Batch [600], Loss: 1.1483\n",
      "2024-12-14 13:49:35,897 - INFO - Epoch [3/5], Batch [700], Loss: 1.2192\n",
      "2024-12-14 13:50:42,247 - INFO - Epoch [3/5], Batch [800], Loss: 1.1690\n",
      "2024-12-14 13:51:47,881 - INFO - Epoch [3/5], Batch [900], Loss: 1.1206\n",
      "2024-12-14 13:52:55,985 - INFO - Epoch [3/5], Batch [1000], Loss: 1.1286\n",
      "2024-12-14 13:54:04,056 - INFO - Epoch [3/5], Batch [1100], Loss: 1.1415\n",
      "2024-12-14 13:55:15,097 - INFO - Epoch [3/5], Batch [1200], Loss: 1.1474\n",
      "2024-12-14 13:56:20,247 - INFO - Epoch [3/5], Batch [1300], Loss: 1.1603\n",
      "2024-12-14 13:57:25,764 - INFO - Epoch [3/5], Batch [1400], Loss: 1.0897\n",
      "2024-12-14 13:58:32,281 - INFO - Epoch [3/5], Batch [1500], Loss: 1.1569\n",
      "2024-12-14 13:59:09,473 - INFO - Evaluating after epoch 3...\n",
      "2024-12-14 13:59:39,062 - INFO - Test Accuracy: 62.14%\n",
      "2024-12-14 14:00:51,733 - INFO - Epoch [4/5], Batch [100], Loss: 1.0557\n",
      "2024-12-14 14:02:09,918 - INFO - Epoch [4/5], Batch [200], Loss: 1.1485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m    106\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n\u001b[0;32m    110\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal evaluation on test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     72\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN loss detected at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m clip_gradients(model, clip_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)  \n\u001b[0;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Aadil Nazir\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32 # bcz i am working on CPU\n",
    "learning_rate = 0.001 # You can incraesed Decrease LR acc... to Model Accuarcy\n",
    "num_epochs = 5   # I started with less Epochs\n",
    "\n",
    "# Data preparation with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalization for CIFAR-10\n",
    "])\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Device setup (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# Model, Loss, and Optimizer setup\n",
    "model = models.resnet18(weights=None)  \n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  \n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce lr every 5 epochs\n",
    "\n",
    "\n",
    "def clip_gradients(model, clip_value):\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                logging.error(f\"NaN loss detected at epoch {epoch + 1}, batch {i}\")\n",
    "                return\n",
    "\n",
    "            loss.backward()\n",
    "            clip_gradients(model, clip_value=1.0)  \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 0:  # Log every 100 batches\n",
    "                avg_loss = running_loss / 100\n",
    "                logging.info(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{i}], Loss: {avg_loss:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        scheduler.step()  # Update learning rate\n",
    "        logging.info(f\"Evaluating after epoch {epoch + 1}...\")\n",
    "        evaluate(model, test_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    logging.info(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Start training\n",
    "logging.info(\"Starting training...\")\n",
    "train(model, train_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "\n",
    "# Final evaluation\n",
    "logging.info(\"Final evaluation on test data...\")\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c75fd-d0bc-43ff-b05a-b7b6dad0afcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
