{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02fd2613-aa70-4e4f-b1d0-2f4a5f4e5117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-14 11:47:36,716] A new study created in memory with name: no-name-54fb0611-7441-4d3f-bad7-bedbedcc88cb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: optimizer=sgd, lr=0.0028299879076974566, batch_size=39\n",
      "Files already downloaded and verified\n",
      "Starting epoch 1...\n",
      "Batch 0/1283...\n",
      "Batch 10/1283...\n",
      "Batch 20/1283...\n",
      "Batch 30/1283...\n",
      "Batch 40/1283...\n",
      "Batch 50/1283...\n",
      "Batch 60/1283...\n",
      "Batch 70/1283...\n",
      "Batch 80/1283...\n",
      "Batch 90/1283...\n",
      "Batch 100/1283...\n",
      "Batch 110/1283...\n",
      "Batch 120/1283...\n",
      "Batch 130/1283...\n",
      "Batch 140/1283...\n",
      "Batch 150/1283...\n",
      "Batch 160/1283...\n",
      "Batch 170/1283...\n",
      "Batch 180/1283...\n",
      "Batch 190/1283...\n",
      "Batch 200/1283...\n",
      "Batch 210/1283...\n",
      "Batch 220/1283...\n",
      "Batch 230/1283...\n",
      "Batch 240/1283...\n",
      "Batch 250/1283...\n",
      "Batch 260/1283...\n",
      "Batch 270/1283...\n",
      "Batch 280/1283...\n",
      "Batch 290/1283...\n",
      "Batch 300/1283...\n",
      "Batch 310/1283...\n",
      "Batch 320/1283...\n",
      "Batch 330/1283...\n",
      "Batch 340/1283...\n",
      "Batch 350/1283...\n",
      "Batch 360/1283...\n",
      "Batch 370/1283...\n",
      "Batch 380/1283...\n",
      "Batch 390/1283...\n",
      "Batch 400/1283...\n",
      "Batch 410/1283...\n",
      "Batch 420/1283...\n",
      "Batch 430/1283...\n",
      "Batch 440/1283...\n",
      "Batch 450/1283...\n",
      "Batch 460/1283...\n",
      "Batch 470/1283...\n",
      "Batch 480/1283...\n",
      "Batch 490/1283...\n",
      "Batch 500/1283...\n",
      "Batch 510/1283...\n",
      "Batch 520/1283...\n",
      "Batch 530/1283...\n",
      "Batch 540/1283...\n",
      "Batch 550/1283...\n",
      "Batch 560/1283...\n",
      "Batch 570/1283...\n",
      "Batch 580/1283...\n",
      "Batch 590/1283...\n",
      "Batch 600/1283...\n",
      "Batch 610/1283...\n",
      "Batch 620/1283...\n",
      "Batch 630/1283...\n",
      "Batch 640/1283...\n",
      "Batch 650/1283...\n",
      "Batch 660/1283...\n",
      "Batch 670/1283...\n",
      "Batch 680/1283...\n",
      "Batch 690/1283...\n",
      "Batch 700/1283...\n",
      "Batch 710/1283...\n",
      "Batch 720/1283...\n",
      "Batch 730/1283...\n",
      "Batch 740/1283...\n",
      "Batch 750/1283...\n",
      "Batch 760/1283...\n",
      "Batch 770/1283...\n",
      "Batch 780/1283...\n",
      "Batch 790/1283...\n",
      "Batch 800/1283...\n",
      "Batch 810/1283...\n",
      "Batch 820/1283...\n",
      "Batch 830/1283...\n",
      "Batch 840/1283...\n",
      "Batch 850/1283...\n",
      "Batch 860/1283...\n",
      "Batch 870/1283...\n",
      "Batch 880/1283...\n",
      "Batch 890/1283...\n",
      "Batch 900/1283...\n",
      "Batch 910/1283...\n",
      "Batch 920/1283...\n",
      "Batch 930/1283...\n",
      "Batch 940/1283...\n",
      "Batch 950/1283...\n",
      "Batch 960/1283...\n",
      "Batch 970/1283...\n",
      "Batch 980/1283...\n",
      "Batch 990/1283...\n",
      "Batch 1000/1283...\n",
      "Batch 1010/1283...\n",
      "Batch 1020/1283...\n",
      "Batch 1030/1283...\n",
      "Batch 1040/1283...\n",
      "Batch 1050/1283...\n",
      "Batch 1060/1283...\n",
      "Batch 1070/1283...\n",
      "Batch 1080/1283...\n",
      "Batch 1090/1283...\n",
      "Batch 1100/1283...\n",
      "Batch 1110/1283...\n",
      "Batch 1120/1283...\n",
      "Batch 1130/1283...\n",
      "Batch 1140/1283...\n",
      "Batch 1150/1283...\n",
      "Batch 1160/1283...\n",
      "Batch 1170/1283...\n",
      "Batch 1180/1283...\n",
      "Batch 1190/1283...\n",
      "Batch 1200/1283...\n",
      "Batch 1210/1283...\n",
      "Batch 1220/1283...\n",
      "Batch 1230/1283...\n",
      "Batch 1240/1283...\n",
      "Batch 1250/1283...\n",
      "Batch 1260/1283...\n",
      "Batch 1270/1283...\n",
      "Batch 1280/1283...\n",
      "Epoch 1 completed. Loss: 0.9718594900909477\n",
      "Starting epoch 2...\n",
      "Batch 0/1283...\n",
      "Batch 10/1283...\n",
      "Batch 20/1283...\n",
      "Batch 30/1283...\n",
      "Batch 40/1283...\n",
      "Batch 50/1283...\n",
      "Batch 60/1283...\n",
      "Batch 70/1283...\n",
      "Batch 80/1283...\n",
      "Batch 90/1283...\n",
      "Batch 100/1283...\n",
      "Batch 110/1283...\n",
      "Batch 120/1283...\n",
      "Batch 130/1283...\n",
      "Batch 140/1283...\n",
      "Batch 150/1283...\n",
      "Batch 160/1283...\n",
      "Batch 170/1283...\n",
      "Batch 180/1283...\n",
      "Batch 190/1283...\n",
      "Batch 200/1283...\n",
      "Batch 210/1283...\n",
      "Batch 220/1283...\n",
      "Batch 230/1283...\n",
      "Batch 240/1283...\n",
      "Batch 250/1283...\n",
      "Batch 260/1283...\n",
      "Batch 270/1283...\n",
      "Batch 280/1283...\n",
      "Batch 290/1283...\n",
      "Batch 300/1283...\n",
      "Batch 310/1283...\n",
      "Batch 320/1283...\n",
      "Batch 330/1283...\n",
      "Batch 340/1283...\n",
      "Batch 350/1283...\n",
      "Batch 360/1283...\n",
      "Batch 370/1283...\n",
      "Batch 380/1283...\n",
      "Batch 390/1283...\n",
      "Batch 400/1283...\n",
      "Batch 410/1283...\n",
      "Batch 420/1283...\n",
      "Batch 430/1283...\n",
      "Batch 440/1283...\n",
      "Batch 450/1283...\n",
      "Batch 460/1283...\n",
      "Batch 470/1283...\n",
      "Batch 480/1283...\n",
      "Batch 490/1283...\n",
      "Batch 500/1283...\n",
      "Batch 510/1283...\n",
      "Batch 520/1283...\n",
      "Batch 530/1283...\n",
      "Batch 540/1283...\n",
      "Batch 550/1283...\n",
      "Batch 560/1283...\n",
      "Batch 570/1283...\n",
      "Batch 580/1283...\n",
      "Batch 590/1283...\n",
      "Batch 600/1283...\n",
      "Batch 610/1283...\n",
      "Batch 620/1283...\n",
      "Batch 630/1283...\n",
      "Batch 640/1283...\n",
      "Batch 650/1283...\n",
      "Batch 660/1283...\n",
      "Batch 670/1283...\n",
      "Batch 680/1283...\n",
      "Batch 690/1283...\n",
      "Batch 700/1283...\n",
      "Batch 710/1283...\n",
      "Batch 720/1283...\n",
      "Batch 730/1283...\n",
      "Batch 740/1283...\n",
      "Batch 750/1283...\n",
      "Batch 760/1283...\n",
      "Batch 770/1283...\n",
      "Batch 780/1283...\n",
      "Batch 790/1283...\n",
      "Batch 800/1283...\n",
      "Batch 810/1283...\n",
      "Batch 820/1283...\n",
      "Batch 830/1283...\n",
      "Batch 840/1283...\n",
      "Batch 850/1283...\n",
      "Batch 860/1283...\n",
      "Batch 870/1283...\n",
      "Batch 880/1283...\n",
      "Batch 890/1283...\n",
      "Batch 900/1283...\n",
      "Batch 910/1283...\n",
      "Batch 920/1283...\n",
      "Batch 930/1283...\n",
      "Batch 940/1283...\n",
      "Batch 950/1283...\n",
      "Batch 960/1283...\n",
      "Batch 970/1283...\n",
      "Batch 980/1283...\n",
      "Batch 990/1283...\n",
      "Batch 1000/1283...\n",
      "Batch 1010/1283...\n",
      "Batch 1020/1283...\n",
      "Batch 1030/1283...\n",
      "Batch 1040/1283...\n",
      "Batch 1050/1283...\n",
      "Batch 1060/1283...\n",
      "Batch 1070/1283...\n",
      "Batch 1080/1283...\n",
      "Batch 1090/1283...\n",
      "Batch 1100/1283...\n",
      "Batch 1110/1283...\n",
      "Batch 1120/1283...\n",
      "Batch 1130/1283...\n",
      "Batch 1140/1283...\n",
      "Batch 1150/1283...\n",
      "Batch 1160/1283...\n",
      "Batch 1170/1283...\n",
      "Batch 1180/1283...\n",
      "Batch 1190/1283...\n",
      "Batch 1200/1283...\n",
      "Batch 1210/1283...\n",
      "Batch 1220/1283...\n",
      "Batch 1230/1283...\n",
      "Batch 1240/1283...\n",
      "Batch 1250/1283...\n",
      "Batch 1260/1283...\n",
      "Batch 1270/1283...\n",
      "Batch 1280/1283...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-14 12:41:53,635] Trial 0 finished with value: 0.6268795250057709 and parameters: {'optimizer': 'sgd', 'learning_rate': 0.0028299879076974566, 'batch_size': 39}. Best is trial 0 with value: 0.6268795250057709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Loss: 0.6268795250057709\n",
      "Best hyperparameters:  {'optimizer': 'sgd', 'learning_rate': 0.0028299879076974566, 'batch_size': 39}\n"
     ]
    }
   ],
   "source": [
    "# Aadil Nazir\n",
    "import optuna\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def objective(trial):\n",
    "    # HP search space\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'sgd'])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "\n",
    "    print(f\"Trial {trial.number}: optimizer={optimizer_name}, lr={learning_rate}, batch_size={batch_size}\")\n",
    "\n",
    "    # Data transformation and loading\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Model setup\n",
    "    model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT) # weight updated \n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) if optimizer_name == 'adam' else \\\n",
    "                optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 2\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}...\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            if i % 10 == 0:  # log every 10 batches\n",
    "                print(f\"Batch {i}/{len(trainloader)}...\")\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss: {running_loss / len(trainloader)}\")\n",
    "\n",
    "    return running_loss / len(trainloader)\n",
    "\n",
    "#optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5, timeout=300) # increased\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d0195-7d73-491c-b6f7-6cc6dd4628d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
