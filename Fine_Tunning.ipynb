{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d032827-5632-49f6-8419-6f988b3b2943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 14:01:38,365 - INFO - Using device: cpu\n",
      "2024-12-14 14:01:40,430 - INFO - Fine-tuning the pre-trained model...\n",
      "2024-12-14 14:04:28,374 - INFO - Epoch [1/10], Batch [100], Loss: 1.8689\n",
      "2024-12-14 14:07:56,087 - INFO - Epoch [1/10], Batch [200], Loss: 1.5549\n",
      "2024-12-14 14:11:50,847 - INFO - Epoch [1/10], Batch [300], Loss: 1.4071\n",
      "2024-12-14 14:15:29,767 - INFO - Epoch [1/10], Batch [400], Loss: 1.3384\n",
      "2024-12-14 14:19:06,819 - INFO - Epoch [1/10], Batch [500], Loss: 1.3049\n",
      "2024-12-14 14:22:53,448 - INFO - Epoch [1/10], Batch [600], Loss: 1.2098\n",
      "2024-12-14 14:26:36,442 - INFO - Epoch [1/10], Batch [700], Loss: 1.1723\n",
      "2024-12-14 14:29:42,278 - INFO - Evaluating the model after epoch 1...\n",
      "2024-12-14 14:30:29,713 - INFO - Test Accuracy: 54.01%\n",
      "2024-12-14 14:34:32,720 - INFO - Epoch [2/10], Batch [100], Loss: 2.3324\n",
      "2024-12-14 14:38:07,707 - INFO - Epoch [2/10], Batch [200], Loss: 2.0086\n",
      "2024-12-14 14:41:50,206 - INFO - Epoch [2/10], Batch [300], Loss: 1.8264\n"
     ]
    }
   ],
   "source": [
    "# Aadil Nazir UE208001 UIET PU Chandigarh...\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Fix \n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64  # Increased batch size \n",
    "learning_rate = 0.001  \n",
    "num_epochs = 10  # Increased\n",
    "\n",
    "# Data preparation with additional augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Added color jitter\n",
    "    transforms.RandomAffine(10, shear=10),  # Added random affine for more augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalization for CIFAR-10\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Device setup (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# Load a pre-trained ResNet18 model with the correct weight enum\n",
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # or ResNet18_Weights.DEFAULT\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  \n",
    "model = model.to(device)  \n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # Unfreeze all layers\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce lr every 5 epochs\n",
    "\n",
    "# Training function\n",
    "def train_finetune(model, train_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                avg_loss = running_loss / 100\n",
    "                logging.info(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{i}], Loss: {avg_loss:.4f}\")\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Evaluate model after each epoch\n",
    "        logging.info(f\"Evaluating the model after epoch {epoch+1}...\")\n",
    "        test(model, test_loader)\n",
    "\n",
    "# Testing function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    logging.info(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Fine-tune the pre-trained model\n",
    "logging.info(\"Fine-tuning the pre-trained model...\")\n",
    "train_finetune(model, train_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "\n",
    "# Test the fine-tuned model\n",
    "logging.info(\"Evaluating the fine-tuned model...\")\n",
    "test(model, test_loader)\n",
    "\n",
    "# Train the model from scratch for comparison\n",
    "logging.info(\"Training a new model from scratch...\")\n",
    "model_scratch = models.resnet18(weights=None) \n",
    "model_scratch.fc = nn.Linear(model_scratch.fc.in_features, 10)\n",
    "model_scratch = model_scratch.to(device)\n",
    "\n",
    "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler_scratch = optim.lr_scheduler.StepLR(optimizer_scratch, step_size=5, gamma=0.1)\n",
    "# fine_tune model\n",
    "train_finetune(model_scratch, train_loader, criterion, optimizer_scratch, scheduler_scratch, num_epochs)\n",
    "\n",
    "# Test the scratch model\n",
    "logging.info(\"Evaluating the Model Trained from Scratch...\")\n",
    "test(model_scratch, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf91c14-4a2e-4e68-9b3e-5ff21f454e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
